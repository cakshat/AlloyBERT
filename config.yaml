epochs: 50
batch_size: 4
stage: finetune  # pretrain, finetune
notes: 
debug: false
load_pretrained: false

optim:
  lr: 1.0e-5

sch:
  name: linear  # constant, linear
  warmup_steps: 0
  # factor: 0.1
  # patience: 4

paths:
  train_data: 'data/ys_clean/tr.pkl'
  val_data: 'data/ys_clean/vl.pkl'
  tokenizer: 'roberta-base'
  pretrained: 
